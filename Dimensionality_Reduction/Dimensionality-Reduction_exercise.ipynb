{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "There can be many reasons why we may want to reduce the number of features in our data.  Computation time may be a a concern for particularly large datasets, or we may want to use an algorithm that requires uncorrelated inputs.  Whatever our reasons may be, there are a few tools that we have at our disposal which we will explore in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "PCA is likely to end up being your main workhorse when you want to reduce the number of features or uncorrelate your inputs.  Recall that PCA works by finding a set of principal axis in your data that which maximizes the variance, and as a bonus, you get a reduced set of uncorrelated features which can be used to express your data.  Most software packages perform PCA via a Singular Value Decomposition given by $$\\mathbf {M} =\\mathbf {U} {\\boldsymbol {\\Sigma }}\\mathbf {V} ^{*}$$ Where $\\mathbf{V} ^{*}$ is a matrix containing the principal components.\n",
    "\n",
    "You may notice that PCA finds a linear mapping from the full data space, to the reduced component space.  But what if we wanted to use a non-linear mapping?  To accomplish this, we can use a non-linear kernel in our PCA which is called Kernel PCA.\n",
    "\n",
    "In this exercise you'll explore these concepts using both generated and real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Generate a set of 300 noisy random data points along the line $y = x + \\epsilon$.  Compute the mean, variance, and correlation of your data and plot.\n",
    "1. Compute the PCA of the randomly generated data above, printing the explained variance of each component.  Project onto the new axis, plot, and print the variance, mean, and correlation matrix of the new data.  Comment on your results.\n",
    "1. Now add a third column to your randomly generated data that is just a copy of one of already existing columns, perform a PCA, and comment on your results.\n",
    "1. Head over to the [Machine Learning Repository](https://archive.ics.uci.edu/ml) and download the [Communities and Crime](https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime) data set, put it into a dataframe, dropping the `state`, `county`, `community`, `communityname` columns, and break down into training and test sets. `ViolentCrimesPerPop` will be the column we want to predict later on.\n",
    "1. Perform a PCA on the data and make a plot of the the explained variance versus number of principal components.  What seems like a good choice of components?\n",
    "1. Using the number of components you found above, fit two different machine learning algorithms to the data, computing training/test error.  Comment on your results.\n",
    "1. Now fit the same models used in part (6), but this time using the full data set.  Comment on your results.\n",
    "1. Classify the response into 3 categories, `High`, `Med`, and `Low`, use PCA to reduce the data to 2 dimensions, and then generate a plot to show the latent clusters in the data.\n",
    "1. Perform a KPCA, using `linear`, `rbf`, and `poly` kernels, on the data, using the same number of components as above, and fit the same machine learning algorithms and in part (6).  Comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis\n",
    "Recall that Linear Discriminant Analysis is a classification algorithm which essentially finds a hyperplane which separates the data, thereby providing classification.  As it turns out, this same technique can also be used for dimensionality reduction in a similar manner as PCA.  In this exercise, you'll use the `Digits` dataset in `sklearn`, comparing your results with PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the `Digits` data from `sklearn`, perform a LDA variable reduction, projecting the data onto 2 dimensions, and generate a plot, showing the latent clusters in the data.  Comment on your results.\n",
    "1. Determine a reasonable number of components for dimensionality reduction on the data.  Comment on your results.\n",
    "1. Split your data into training and test sets, perform feature reduction using the number of features determined in part (2), and fit two machine learning algorithms of your choice to the data, computing the training and test accuracies.  Comment on your results.\n",
    "1. Perform part (3) again, but this time with the full data set and comment on your results.\n",
    "1. Perform dimensionality reduction using PCA, and fit the same models you used in parts (3) and (4).  Comment on your results.\n",
    "1. Perform dimensionality reduction using KPCA with `rbf` and `poly` kernels, the same number of components as above, and fit the same models you used in parts (3) and (4).  Comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifolds\n",
    "With the exception of Kernel PCA, the techniques we have explored so far, utilize only linear mappings.  Another non-linear mapping are generically called Manifold Learning which are fairly advanced, graduate level, mathematical topics.  Luckily for us, there are a number of methods made available in `sklearn` which will we use in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Head over to the [Machine Learning Repository](https://archive.ics.uci.edu/ml) and download the [MADELON](https://archive.ics.uci.edu/ml/datasets/MADELON) training data and labels, put it into a dataframe, and break into training and test sets.\n",
    "1. Perform variable reduction, to two components, via Multidimensional Scaling, and plot the data to show the latent clusters.  Comment on your results.\n",
    "1. Perform variable reduction, to two components, via Isomap, and plot the data to show the latent clusters.  Comment on your results.\n",
    "1. Perform variable reduction, to two components, via Locally Linear Embedding, and plot the data to show the latent clusters.  Comment on your results.\n",
    "1. Perform variable reduction, to two components, via TSNE, and plot the data to show the latent clusters.  Comment on your results.\n",
    "1. Perform variable reduction, to two components, via PCA, and plot the data to show the latent clusters.  Comment on your results.\n",
    "1. Using a machine learning algorithm of your choice, fit a model to the data after performing feature reduction using Isomap, LLE, and PCA.  Report test and training accuracies and comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
