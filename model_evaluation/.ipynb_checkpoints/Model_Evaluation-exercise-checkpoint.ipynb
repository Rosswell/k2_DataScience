{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline and Feature Unions\n",
    "It is always a good decision to make your code as readable as possible.  Not only so that others can pick it up and use it easily, but so that you don't end up hating your past self when you take a look at something you wrote several months prior.  Pipelines are a great tool for integrating and reusing a series of data transformations and fits into a workflow. In this exercise you'll build some pipelines and feature unions using the Concrete Compressive Data Set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Head over to the [Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php), download the [Concrete Compressive Data Set](http://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/), put it into a dataframe, and split into training and test sets.  Be sure to familiarize yourself with the data before proceeding.\n",
    "1. Build a pipeline for polynomial fitting, fit polynomials of degree 1 to the number of features, and plot your training and testing errors for each.  Comment on your results\n",
    "1. Build a pipeline that will perform feature selection on the dataset using the F-Statistic, producing the same plots as in part (2).  Comment on your results.\n",
    "1. Build a pipeline that standardizes your data, performs feature selection via regularization, and then fits a model of your choice.  Produce the same plots as above and comment on your results.\n",
    "1. Create two pipelines for feature selection of a technique of your choice, scaling the data before hand.  Then, join these two pipelines with a `FeatureUnion`, and fit a polynomial model, also in a pipeline.  Comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "It is very important that you have more than one tool in your toolbox for evaluating the usefulness of your model, as in different context, different metrics are preferred.  For example, suppose a new medical test is developed for detecting cancer that has a 0.25 probability of incorrectly labeling a patient of having cancer when they in fact do not, but a 0.001 probability of labeling a cancer patient as cancer free.  With this sort of test, you can be sure that those who do have cancer will almost certainly be classified correctly, but a positive does not necessarily mean that the patient has cancer, meaning additional tests are in order.  These metrics have different names and depending on the situation, you may be interested in minimizing different quantities, which is the topic we will explore in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Head over the [Machine Learning Repository](http://archive.ics.uci.edu/ml/) and download the [Breast Cancer Diagnostic Dataset](http://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29), put it in a dataframe, and split into testing and training sets.\n",
    "1. Using a classification algorithm of your choice, fit a model to the data and predict the results, making your results as accurate as possible.\n",
    "1. Using your model in part (2), compute the following quantities, without using `skelarn.metrics`.\n",
    "    - True Positives\n",
    "    - True Negatives\n",
    "    - False Positives\n",
    "    - False Negatives\n",
    "1.  Using your results in part (3), compute the following quantities.\n",
    "    - Sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "    - Specificity or true negative rate (TNR)\n",
    "    - Miss rate or false negative rate (FNR)\n",
    "    - Fall-out or false positive rate (FPR)\n",
    "    - Precision\n",
    "    - F1\n",
    "    - Accuracy\n",
    "1. Check your results in part (4) using `sklearn`.\n",
    "1. Plot the precision and recall curve for your fit.\n",
    "1. Plot the ROC curve for your fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning and Evaluation Curves, Hyperparameter Tuning, and Bootstrapping\n",
    "A problem that you will see crop up time and time again in Data Science is overfitting.  Much like how people can sometimes see structure where there is none, machine learning algorithms suffer from the same.  If you have overfit your model to your data, it has learned a \"pattern\" in the noise rather than the signal you were looking for, and thus will not generalize well to data it has not seen.\n",
    "\n",
    "Consider the LASSO Regression model which we have used previously.  Like all parametric models, fitting a LASSO Regression model can be reduced to the problem of finding a set of $\\hat{\\theta_i}$ which minimize the cost function given the data.  But notice that unlike a standard linear regression model, LASSO Regression has an additional regularization parameter.  The result of this is that the $\\hat{\\theta_i}$ are dependent not only on our data, but also on this additional hyperparameter.\n",
    "\n",
    "So now we have three different problems to juggle while we are fitting our models: overfitting, underfitting, and hyperparameter tuning.  A common technique to deal with all three is cross-validation which we will explore in this exercise.\n",
    "\n",
    "Also, you may find yourself in a situation where you do not have enough data to build a good model.  Again, with some simple assumptions, we can \"pull ourselves up by our bootstraps\", and get by reasonably well with the data we have using a method called bootstrapping.\n",
    "\n",
    "In this exercise, you'll explore these topics using the `wine` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Head over to the [Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php), download the [Wine Dataset](http://archive.ics.uci.edu/ml/datasets/Wine), and put it in a dataframe, being sure to label the columns properly.\n",
    "1. Separate your data into train and test sets, of portions ranging from `test_size = 0.99` to `test_size = 0.01`, fitting a logistic regression model to each and computing the training and test errors.  Plot the errors of the training and test sets on the same plot.  Do this without using `sklearn.model_select.learning_curve`.  Comment on your results.\n",
    "1. Repeat part (2) but this time using `sklearn.model_select.learning_curve`.  Comment on your results.\n",
    "1. Use K-Fold cross validation to tune the regularization strength parameter of the logistic regression.  Do this for values of `k` ranging from `2` to `5`, make relevant  plots, and comment on your results.  Do this without using `sklearn.model_select.evaluation_curve`.\n",
    "1. Tune the regularization strength parameter as in part (4), but this time using `sklearn.model_select.evaluation_curve`.  Comment on your results.\n",
    "1. Fit another classification algorithm to the data, tuning the parameter using LOOCV.  Comment on your results.\n",
    "1. Suppose that the wine data the we received was incomplete, containing, say, only 20% of the full set, but due to a fast approaching deadline, we need to still compute some statistics and fit a model.  Use bootstrapping to compute the mean and variance of the features, and fit the classification model you used in part (4), comparing and commenting on your results with those from the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "All that has been within the previous sections is part of the much broader topic known as model selection.  Model selection is not only about choosing the right machine learning algorithm to use, but also about tuning parameters while keeping overfitting and scalability issues in mind.  In this exercise, we'll build models for a couple different datasets, using all of the concepts you've worked with previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Classification\n",
    "1. Head over to the [Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php), download the [Mushroom Data Set](http://archive.ics.uci.edu/ml/datasets/Mushroom), and put it into a dataframe.  Be sure to familiarize yourself with the data before proceeding.  Break the data into training and testing sets.  Be sure to keep this testing set the same for the duration of this exercise as we will be using to test various algorithms!\n",
    "1. Fit a machine learning algorithm of your choice to the data, tuning hyperparameters using the Better Holdout Method.  Generate training and validation plots and comment on your results.\n",
    "1. Repeat part (2), this time using cross validation.  Comment on your results.\n",
    "1. Repeat part (3) using a different machine learning algorithm.  Comment on your results.\n",
    "1. Which ever of your two algorithms in parts (3) and (4) performed more poorly, perform a variable selection to see if you can improve your results.\n",
    "1. Pick a classification algorithm that has at least two hyperparameters and tune them using `GridSeachCV`.  Comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "1. Head over to the [Machine Learning Repository](http://archive.ics.uci.edu/ml/index.php), download the [Parkinson's Telemonitoring Data Set](http://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring), and put it into a dataframe.  Be sure to familiarize yourself with the data before proceeding, removing the columns related to time, age, and sex.  We're going to be predicting `motor_UPDRS`, so drop the `total_UPDRS` variable as well.  Break the data into training and testing sets.  Be sure to keep this testing set the same for the duration of this exercise as we will be using to test various algorithms!\n",
    "1. Fit a machine learning algorithm of your choice to the data, tuning hyperparameters using the Better Holdout Method. Generate training and validation plots and comment on your results.\n",
    "1. Repeat part (2), this time using cross validation.  Comment on your results.\n",
    "1. Repeat part (3) using a different machine learning algorithm.  Comment on your results.\n",
    "1. Which ever of your two algorithms in parts (3) and (4) performed more poorly, perform a variable selection to see if you can improve your results.\n",
    "1. Pick a regression algorithm that has at least two hyperparameters and tune them using `GridSeachCV`.  Comment on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
